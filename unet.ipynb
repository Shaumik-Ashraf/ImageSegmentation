{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>HuBMAP: Hacking the Kidney - Training and Inference</center></h1>\n",
    "<h2><center>Revised for DL Project</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Modelling GPU Offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using **pytorch** implementation of **UNet** Model implemented in **https://github.com/qubvel/segmentation_models.pytorch** and this is getting installed offline.\n",
    "\n",
    "##### Takes ~15 minutes to train on 8 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkdir -p /tmp/pip/cache/\n",
    "#cp ../input/segmentationmodelspytorch/segmentation_models/efficientnet_pytorch-0.6.3.xyz /tmp/pip/cache/efficientnet_pytorch-0.6.3.tar.gz\n",
    "#cp ../input/segmentationmodelspytorch/segmentation_models/pretrainedmodels-0.7.4.xyz /tmp/pip/cache/pretrainedmodels-0.7.4.tar.gz\n",
    "#cp ../input/segmentationmodelspytorch/segmentation_models/segmentation-models-pytorch-0.1.2.xyz /tmp/pip/cache/segmentation_models_pytorch-0.1.2.tar.gz\n",
    "#cp ../input/segmentationmodelspytorch/segmentation_models/timm-0.1.20-py3-none-any.whl /tmp/pip/cache/\n",
    "#cp ../input/segmentationmodelspytorch/segmentation_models/timm-0.2.1-py3-none-any.whl /tmp/pip/cache/\n",
    "#pip install --no-index --find-links /tmp/pip/cache/ efficientnet-pytorch\n",
    "#pip install --no-index --find-links /tmp/pip/cache/ segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import albumentations as A\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import zipfile\n",
    "import time\n",
    "import random\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from segmentation_models_pytorch.unet import Unet\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b0-355c32eb.pth /root/.cache/torch/hub/checkpoints/\n",
    "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b1-f1951068.pth /root/.cache/torch/hub/checkpoints/\n",
    "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b2-8bb594d6.pth /root/.cache/torch/hub/checkpoints/\n",
    "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b3-5fb5a3c3.pth /root/.cache/torch/hub/checkpoints/\n",
    "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b4-6ed6700e.pth /root/.cache/torch/hub/checkpoints/\n",
    "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b5-b6417697.pth /root/.cache/torch/hub/checkpoints/\n",
    "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b6-c76e70fd.pth /root/.cache/torch/hub/checkpoints/\n",
    "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b7-dcc49843.pth /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "FOLDS = 3\n",
    "learning_rate = 5e-4\n",
    "train_batch_size = 4\n",
    "val_batch_size = 4\n",
    "num_workers = 0 # windows\n",
    "\n",
    "images_path = \"data\\\\small\\\\train\"\n",
    "masks_path = \"data\\\\small\\\\masks\"\n",
    "\n",
    "seed = 42;\n",
    "sz = 256  \n",
    "reduce = 4\n",
    "TH = 0.39 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc2mask(encs, shape):\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for m,enc in enumerate(encs):\n",
    "        if isinstance(enc,np.float) and np.isnan(enc): continue\n",
    "        s = enc.split()\n",
    "        for i in range(len(s)//2):\n",
    "            start = int(s[2*i]) - 1\n",
    "            length = int(s[2*i+1])\n",
    "            img[start:start+length] = 1 + m\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def mask2enc(mask, n=1):\n",
    "    pixels = mask.T.flatten()\n",
    "    encs = []\n",
    "    for i in range(1,n+1):\n",
    "        p = (pixels == i).astype(np.int8)\n",
    "        if p.sum() == 0: encs.append(np.nan)\n",
    "        else:\n",
    "            p = np.concatenate([[0], p, [0]])\n",
    "            runs = np.where(p[1:] != p[:-1])[0] + 1\n",
    "            runs[1::2] -= runs[::2]\n",
    "            encs.append(' '.join(str(x) for x in runs))\n",
    "    return encs\n",
    "\n",
    "#https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n",
    "#with bug fix\n",
    "def rle_encode_less_memory(img):\n",
    "    #watch out for the bug\n",
    "    pixels = img.T.flatten()\n",
    "    \n",
    "    # This simplified method requires first and last pixel to be zero\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuBMAPDataset(Dataset):\n",
    "    def __init__(self, ids, phase):\n",
    "        self.ids = ids\n",
    "        if phase=='train':\n",
    "            self.transform = get_train_transform()\n",
    "        else:\n",
    "            self.transform = get_val_transform()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        name = self.ids[idx]\n",
    "        print(name)\n",
    "        img = cv2.imread(os.path.join(images_path, name)).astype(\"float32\")[:,:,::-1] # changed path\n",
    "        img /= 255.\n",
    "        mask = cv2.imread(os.path.join(masks_path, name))[:,:,0:1] # changed path\n",
    "\n",
    "        transformed = self.transform(image=img, mask=mask)\n",
    "        img = transformed['image']\n",
    "        mask = transformed['mask']\n",
    "        img = img.transpose(2,0,1).astype('float32')\n",
    "        mask = mask.transpose(2,0,1).astype('float32')\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "        \n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(),\n",
    "            A.OneOf([\n",
    "                A.RandomContrast(),\n",
    "                A.RandomGamma(),\n",
    "                A.RandomBrightness(),\n",
    "                ], p=0.3),\n",
    "            A.OneOf([\n",
    "                A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "                A.GridDistortion(),\n",
    "                A.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n",
    "                ], p=0.3),\n",
    "            A.ShiftScaleRotate(p=0.2),\n",
    "            A.Resize(256,256,always_apply=True),\n",
    "    ],p=1.)\n",
    "\n",
    "def get_val_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(256,256,always_apply=True),\n",
    "    ],p=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1e2425f28_0019.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1e2425f28_0020.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1e2425f28_0021.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1e2425f28_0038.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1e2425f28_0039.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1e2425f28_0040.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1e2425f28_0041.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1e2425f28_0042.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Paths\n",
       "0  1e2425f28_0019.png\n",
       "1  1e2425f28_0020.png\n",
       "2  1e2425f28_0021.png\n",
       "3  1e2425f28_0038.png\n",
       "4  1e2425f28_0039.png\n",
       "5  1e2425f28_0040.png\n",
       "6  1e2425f28_0041.png\n",
       "7  1e2425f28_0042.png"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_list = os.listdir(images_path) # changed path\n",
    "dir_df = pd.DataFrame(directory_list, columns=['Image_Paths'])\n",
    "dir_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_valid_dataloader(df, fold):\n",
    "    train_ids = df.loc[~df.Folds.isin(fold), \"Image_Paths\"].values\n",
    "    val_ids = df.loc[df.Folds.isin(fold), \"Image_Paths\"].values\n",
    "    train_ds = HuBMAPDataset(train_ids, \"train\")\n",
    "    val_ds = HuBMAPDataset(val_ids, \"val\")\n",
    "    train_loader = DataLoader(train_ds, batch_size=train_batch_size, pin_memory=True, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_ds, batch_size=val_batch_size, pin_memory=True, shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuBMAP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HuBMAP, self).__init__()\n",
    "        self.cnn_model = Unet('efficientnet-b5', encoder_weights=\"imagenet\", classes=1, activation=None)\n",
    "        #self.cnn_model.decoder.blocks.append(self.cnn_model.decoder.blocks[-1])\n",
    "        #self.cnn_model.decoder.blocks[-2] = self.cnn_model.decoder.blocks[-3]\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        img_segs = self.cnn_model(imgs)\n",
    "        return img_segs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://wikimedia.org/api/rest_v1/media/math/render/svg/80f87a71d3a616a0939f5360cec24d702d2593a2'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return dice\n",
    "    \n",
    "    \n",
    "    \n",
    "class DiceBCELoss(nn.Module):\n",
    "    # Formula Given above.\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).mean()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.mean() + targets.mean() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HuBMAPLoss(images, targets, model, device):\n",
    "    model.to(device)\n",
    "    images = images.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(images)\n",
    "    criterion = DiceBCELoss()\n",
    "    loss = criterion(outputs, targets)\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, device, optimizer, scheduler, trainloader):\n",
    "    model.train()\n",
    "    t = time.time()\n",
    "    total_loss = 0\n",
    "    for step, (images, targets) in enumerate(trainloader):\n",
    "        loss, outputs = HuBMAPLoss(images, targets, model, device)\n",
    "        loss.backward()\n",
    "        if ((step+1)%4==0 or (step+1)==len(trainloader)):\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        loss = loss.detach().item()\n",
    "        total_loss += loss\n",
    "        if ((step+1)%10==0 or (step+1)==len(trainloader)):\n",
    "            print(\n",
    "                    f'epoch {epoch} train step {step+1}/{len(trainloader)}, ' + \\\n",
    "                    f'loss: {total_loss/len(trainloader):.4f}, ' + \\\n",
    "                    f'time: {(time.time() - t):.4f}', end= '\\r' if (step + 1) != len(trainloader) else '\\n'\n",
    "                )\n",
    "\n",
    "            \n",
    "        \n",
    "def valid_one_epoch(epoch, model, device, optimizer, scheduler, validloader):\n",
    "    model.eval()\n",
    "    t = time.time()\n",
    "    total_loss = 0\n",
    "    for step, (images, targets) in enumerate(validloader):\n",
    "        loss, outputs = HuBMAPLoss(images, targets, model, device)\n",
    "        loss = loss.detach().item()\n",
    "        total_loss += loss\n",
    "        if ((step+1)%4==0 or (step+1)==len(validloader)):\n",
    "            scheduler.step(total_loss/len(validloader))\n",
    "        if ((step+1)%10==0 or (step+1)==len(validloader)):\n",
    "            print(\n",
    "                    f'epoch {epoch} valid step {step+1}/{len(validloader)}, ' + \\\n",
    "                    f'loss: {total_loss/len(validloader):.4f}, ' + \\\n",
    "                    f'time: {(time.time() - t):.4f}', end= '\\r' if (step + 1) != len(validloader) else '\\n'\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Folds Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOLDS = 5 # moved to hyperparams\n",
    "gkf = GroupKFold(FOLDS)\n",
    "dir_df['Folds'] = 0\n",
    "for fold, (tr_idx, val_idx) in enumerate(gkf.split(dir_df, groups=dir_df[dir_df.columns[0]].values)):\n",
    "    dir_df.loc[val_idx, 'Folds'] = fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Helper</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(msg, reset = False):\n",
    "    now = time.time();\n",
    "    if not hasattr(timer, \"start\") or reset:\n",
    "        timer.start = now;\n",
    "        diff = 0;\n",
    "    else:\n",
    "        diff = now - timer.start;\n",
    "        \n",
    "    print(f\"{msg} | {diff} seconds\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Real Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training | 0 seconds\n",
      "0 fold loop | 0.0011429786682128906 seconds\n",
      "prepared dataloaders | 0.003138303756713867 seconds\n",
      "initialized model, optim, sched | 0.704103946685791 seconds\n",
      "0 epoch loop | 0.704103946685791 seconds\n",
      "1e2425f28_0038.png\n",
      "1e2425f28_0021.png\n",
      "1e2425f28_0019.png\n",
      "1e2425f28_0041.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ShaumikAshraf\\anaconda3\\envs\\pytorch_cv\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e2425f28_0040.png\n",
      "epoch 0 train step 2/2, loss: 1.1604, time: 208.4845\n",
      "trained epoch | 209.3081133365631 seconds\n",
      "1e2425f28_0020.png\n",
      "1e2425f28_0039.png\n",
      "1e2425f28_0042.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ShaumikAshraf\\anaconda3\\envs\\pytorch_cv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 valid step 1/1, loss: 2.8530, time: 3.3305\n",
      "validated epoch | 212.64127850532532 seconds\n",
      "1 epoch loop | 212.64127850532532 seconds\n",
      "1e2425f28_0021.png\n",
      "1e2425f28_0041.png\n",
      "1e2425f28_0038.png\n",
      "1e2425f28_0040.png\n",
      "1e2425f28_0019.png\n",
      "epoch 1 train step 2/2, loss: 1.1167, time: 213.9873\n",
      "trained epoch | 426.73795199394226 seconds\n",
      "1e2425f28_0020.png\n",
      "1e2425f28_0039.png\n",
      "1e2425f28_0042.png\n",
      "epoch 1 valid step 1/1, loss: 1.9365, time: 3.4015\n",
      "validated epoch | 430.1432571411133 seconds\n",
      "done training | 430.1432571411133 seconds\n",
      "saved model | 430.4766755104065 seconds\n",
      "1 fold loop | 430.4779839515686 seconds\n",
      "prepared dataloaders | 430.4820177555084 seconds\n",
      "initialized model, optim, sched | 431.65240573883057 seconds\n",
      "0 epoch loop | 431.65240573883057 seconds\n",
      "1e2425f28_0021.png\n",
      "1e2425f28_0042.png\n",
      "1e2425f28_0039.png\n",
      "1e2425f28_0040.png\n",
      "1e2425f28_0020.png\n",
      "epoch 0 train step 2/2, loss: 1.3446, time: 217.0770\n",
      "trained epoch | 648.863499879837 seconds\n",
      "1e2425f28_0019.png\n",
      "1e2425f28_0038.png\n",
      "1e2425f28_0041.png\n",
      "epoch 0 valid step 1/1, loss: 1.0125, time: 3.4765\n",
      "validated epoch | 652.3434958457947 seconds\n",
      "1 epoch loop | 652.3434958457947 seconds\n",
      "1e2425f28_0039.png\n",
      "1e2425f28_0020.png\n",
      "1e2425f28_0040.png\n",
      "1e2425f28_0042.png\n",
      "1e2425f28_0021.png\n",
      "epoch 1 train step 2/2, loss: 1.2857, time: 215.1851\n",
      "trained epoch | 867.6873569488525 seconds\n",
      "1e2425f28_0019.png\n",
      "1e2425f28_0038.png\n",
      "1e2425f28_0041.png\n",
      "epoch 1 valid step 1/1, loss: 1.0570, time: 3.3010\n",
      "validated epoch | 870.9934532642365 seconds\n",
      "done training | 870.9934532642365 seconds\n",
      "saved model | 871.3593437671661 seconds\n",
      "2 fold loop | 871.3602862358093 seconds\n"
     ]
    }
   ],
   "source": [
    "timer(\"starting training\", reset = True);\n",
    "for fold, (tr_idx, val_idx) in enumerate(gkf.split(dir_df, groups=dir_df[dir_df.columns[0]].values)):\n",
    "    timer(f\"{fold} fold loop\")\n",
    "    if fold>1:\n",
    "        break\n",
    "        \n",
    "    trainloader, validloader = prepare_train_valid_dataloader(dir_df, [fold])\n",
    "    timer(\"prepared dataloaders\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = HuBMAP().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=1)\n",
    "    timer(\"initialized model, optim, sched\");\n",
    "    \n",
    "    #num_epochs = 15\n",
    "    #num_epochs = 2 # moved to hyperparams\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        timer(f\"{epoch} epoch loop\")\n",
    "        train_one_epoch(epoch, model, device, optimizer, scheduler, trainloader)\n",
    "        timer(\"trained epoch\");\n",
    "        with torch.no_grad():\n",
    "            valid_one_epoch(epoch, model, device, optimizer, scheduler, validloader)\n",
    "        timer(\"validated epoch\");\n",
    "    \n",
    "    timer(\"done training\");\n",
    "    torch.save(model.state_dict(),f'FOLD-{fold}-model.pth')\n",
    "    timer(\"saved model\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
