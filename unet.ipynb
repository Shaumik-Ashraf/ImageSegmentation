{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "unet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmRAHBQswTB4"
      },
      "source": [
        "<h1><center>HuBMAP: Hacking the Kidney - Training and Inference</center></h1>\n",
        "<h2><center>Revised for DL Project</center></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UekpUAtAwTCB"
      },
      "source": [
        "# Pytorch Modelling GPU Offline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgWqOE4TwTCC"
      },
      "source": [
        "We are using **pytorch** implementation of **UNet** Model implemented in **https://github.com/qubvel/segmentation_models.pytorch** and this is getting installed offline.\n",
        "\n",
        "##### Takes ~15 minutes to train on 8 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G306u_a_wTCC"
      },
      "source": [
        "#mkdir -p /tmp/pip/cache/\n",
        "#cp ../input/segmentationmodelspytorch/segmentation_models/efficientnet_pytorch-0.6.3.xyz /tmp/pip/cache/efficientnet_pytorch-0.6.3.tar.gz\n",
        "#cp ../input/segmentationmodelspytorch/segmentation_models/pretrainedmodels-0.7.4.xyz /tmp/pip/cache/pretrainedmodels-0.7.4.tar.gz\n",
        "#cp ../input/segmentationmodelspytorch/segmentation_models/segmentation-models-pytorch-0.1.2.xyz /tmp/pip/cache/segmentation_models_pytorch-0.1.2.tar.gz\n",
        "#cp ../input/segmentationmodelspytorch/segmentation_models/timm-0.1.20-py3-none-any.whl /tmp/pip/cache/\n",
        "#cp ../input/segmentationmodelspytorch/segmentation_models/timm-0.2.1-py3-none-any.whl /tmp/pip/cache/\n",
        "#pip install --no-index --find-links /tmp/pip/cache/ efficientnet-pytorch\n",
        "#pip install --no-index --find-links /tmp/pip/cache/ segmentation-models-pytorch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d6azcTJwTCD"
      },
      "source": [
        "## Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd2mK3IfE_jZ"
      },
      "source": [
        "REDUCED_SET = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDOW75Yy31r5",
        "outputId": "bd12262c-114d-4806-b7db-144b642c582e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoz3HpNh1M4i",
        "outputId": "b3a68459-10b2-48a8-cb43-752ef4fbaa63"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/DLProjectData/hubmap_train_256.zip\" -d \"/content\"\n",
        "#!unzip /content/file.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/DLProjectData/hubmap_train_256.zip\n",
            "replace /content/__MACOSX/._train? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD8fj82jy60s",
        "outputId": "5dc93c31-11e6-4d0f-db54-595a91242d99"
      },
      "source": [
        "!pip install albumentations==0.4.6\n",
        "!pip install segmentation-models-pytorch"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n",
            "Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.7.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.6.3 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.6.3)\n",
            "Requirement already satisfied: timm==0.3.2 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.3.2)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.9.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.41.1)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNWj9uWuwTCD"
      },
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from scipy.ndimage.interpolation import zoom\n",
        "import albumentations as A\n",
        "from torch.nn import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import tifffile as tiff\n",
        "import cv2\n",
        "import zipfile\n",
        "import time\n",
        "import random\n",
        "import albumentations.pytorch\n",
        "\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from segmentation_models_pytorch.unet import Unet\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OCFmfFCwTCE"
      },
      "source": [
        "#mkdir -p /root/.cache/torch/hub/checkpoints/\n",
        "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b0-355c32eb.pth /root/.cache/torch/hub/checkpoints/\n",
        "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b1-f1951068.pth /root/.cache/torch/hub/checkpoints/\n",
        "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b2-8bb594d6.pth /root/.cache/torch/hub/checkpoints/\n",
        "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b3-5fb5a3c3.pth /root/.cache/torch/hub/checkpoints/\n",
        "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b4-6ed6700e.pth /root/.cache/torch/hub/checkpoints/\n",
        "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b5-b6417697.pth /root/.cache/torch/hub/checkpoints/\n",
        "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b6-c76e70fd.pth /root/.cache/torch/hub/checkpoints/\n",
        "#cp ../input/efficientnet-pytorch-b0-b7/efficientnet-b7-dcc49843.pth /root/.cache/torch/hub/checkpoints/"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1uc1kAiwTCE"
      },
      "source": [
        "## Hyperparameters & Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1asHGjRwTCF"
      },
      "source": [
        "num_epochs = 5\n",
        "FOLDS = 5\n",
        "learning_rate = 5e-4\n",
        "train_batch_size = 16\n",
        "val_batch_size = 4\n",
        "num_workers = 0 # windows, for train_loader\n",
        "\n",
        "#images_path = \"/content/drive/MyDrive/DLProjectData/hubmap-256x256/train\"\n",
        "images_path = \"/content/train\"\n",
        "#images_path = \"data\\\\small\\\\train\"\n",
        "\n",
        "#masks_path = \"/content/drive/MyDrive/DLProjectData/hubmap-256x256/masks\"\n",
        "masks_path = \"/content/masks\"\n",
        "#masks_path = \"data\\\\small\\\\masks\"\n",
        "\n",
        "seed = 42;\n",
        "\n",
        "# These aren't even used...\n",
        "# sz = 256  \n",
        "# reduce = 4\n",
        "# TH = 0.39 "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCR0FyHLwTCF"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_everything(seed)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2RJhCNYwTCF"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMjHTv04wTCG"
      },
      "source": [
        "def enc2mask(encs, shape):\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for m,enc in enumerate(encs):\n",
        "        if isinstance(enc,np.float) and np.isnan(enc): continue\n",
        "        s = enc.split()\n",
        "        for i in range(len(s)//2):\n",
        "            start = int(s[2*i]) - 1\n",
        "            length = int(s[2*i+1])\n",
        "            img[start:start+length] = 1 + m\n",
        "    return img.reshape(shape).T\n",
        "\n",
        "def mask2enc(mask, n=1):\n",
        "    pixels = mask.T.flatten()\n",
        "    encs = []\n",
        "    for i in range(1,n+1):\n",
        "        p = (pixels == i).astype(np.int8)\n",
        "        if p.sum() == 0: encs.append(np.nan)\n",
        "        else:\n",
        "            p = np.concatenate([[0], p, [0]])\n",
        "            runs = np.where(p[1:] != p[:-1])[0] + 1\n",
        "            runs[1::2] -= runs[::2]\n",
        "            encs.append(' '.join(str(x) for x in runs))\n",
        "    return encs\n",
        "\n",
        "#https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n",
        "#with bug fix\n",
        "def rle_encode_less_memory(img):\n",
        "    #watch out for the bug\n",
        "    pixels = img.T.flatten()\n",
        "    \n",
        "    # This simplified method requires first and last pixel to be zero\n",
        "    pixels[0] = 0\n",
        "    pixels[-1] = 0\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
        "    runs[1::2] -= runs[::2]\n",
        "    \n",
        "    return ' '.join(str(x) for x in runs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbmWAn08wTCG"
      },
      "source": [
        "class HuBMAPDataset(Dataset):\n",
        "    def __init__(self, ids, phase):\n",
        "        self.ids = ids\n",
        "        if phase=='train':\n",
        "            self.transform = get_train_transform()\n",
        "        else:\n",
        "            self.transform = get_val_transform()\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        name = self.ids[idx]\n",
        "        img = cv2.imread(os.path.join(images_path, name)).astype(\"float32\")[:,:,::-1] # changed path\n",
        "        img /= 255.\n",
        "        mask = cv2.imread(os.path.join(masks_path, name))[:,:,0:1] # changed path\n",
        "\n",
        "        transformed = self.transform(image=img, mask=mask)\n",
        "        img = transformed['image']\n",
        "        mask = transformed['mask']\n",
        "        img = img.transpose(2,0,1).astype('float32')\n",
        "        mask = mask.transpose(2,0,1).astype('float32')\n",
        "        return img, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "        \n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "        A.HorizontalFlip(),\n",
        "            A.OneOf([\n",
        "                A.RandomContrast(),\n",
        "                A.RandomGamma(),\n",
        "                A.RandomBrightness(),\n",
        "                ], p=0.3),\n",
        "            A.OneOf([\n",
        "                A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
        "                A.GridDistortion(),\n",
        "                A.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n",
        "                ], p=0.3),\n",
        "            A.ShiftScaleRotate(p=0.2),\n",
        "            A.Resize(256,256,always_apply=True),\n",
        "    ],p=1.)\n",
        "\n",
        "def get_val_transform():\n",
        "    return A.Compose([\n",
        "        A.Resize(256,256,always_apply=True),\n",
        "    ],p=1.)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD0gatNhwTCH"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbc1CXVYwTCH"
      },
      "source": [
        "directory_list = os.listdir(images_path) # changed path\n",
        "dir_df = pd.DataFrame(directory_list, columns=['Image_Paths'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbGgTRV3o4vH"
      },
      "source": [
        "if REDUCED_SET:\n",
        "    reduced_stem = ['e79de561c', '54f2eec69', 'c68fe75ea', 'afa5e8098', '4ef6695ce', '2f6ecfcdf', 'aaa6a05cc', 'cb2d976f4']\n",
        "    #reduced_stem = ['e79de561c', '54f2eec69', 'c68fe75ea', 'aaa6a05cc']\n",
        "    reduced_list = []\n",
        "\n",
        "    reduced_dict = dict(zip(reduced_stem, [0] * len(reduced_stem)))\n",
        "    print(reduced_dict)\n",
        "\n",
        "    for img_path in directory_list:\n",
        "        for reduced_name in reduced_stem:\n",
        "            if reduced_name in img_path:\n",
        "                reduced_list.append(img_path)\n",
        "                reduced_dict[reduced_name] += 1\n",
        "\n",
        "    print(len(directory_list))\n",
        "    print(reduced_dict)\n",
        "    print(len(reduced_list))\n",
        "\n",
        "    # reduced dir_df\n",
        "    dir_df = pd.DataFrame(reduced_list, columns=['Image_Paths'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I24dC1VxwTCI"
      },
      "source": [
        "def prepare_train_valid_dataloader(df, fold):\n",
        "    train_ids = df.loc[~df.Folds.isin(fold), \"Image_Paths\"].values\n",
        "    val_ids = df.loc[df.Folds.isin(fold), \"Image_Paths\"].values\n",
        "    train_ds = HuBMAPDataset(train_ids, \"train\")\n",
        "    val_ds = HuBMAPDataset(val_ids, \"val\")\n",
        "    train_loader = DataLoader(train_ds, batch_size=train_batch_size, pin_memory=True, shuffle=True, num_workers=num_workers)\n",
        "    val_loader = DataLoader(val_ds, batch_size=val_batch_size, pin_memory=True, shuffle=False, num_workers=num_workers)\n",
        "    return train_loader, val_loader"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AgN7J4rwTCI"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYgeUovZwTCJ"
      },
      "source": [
        "class HuBMAP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HuBMAP, self).__init__()\n",
        "        self.cnn_model = Unet('efficientnet-b5', encoder_weights=\"imagenet\", classes=1, activation=None)\n",
        "        #self.cnn_model.decoder.blocks.append(self.cnn_model.decoder.blocks[-1])\n",
        "        #self.cnn_model.decoder.blocks[-2] = self.cnn_model.decoder.blocks[-3]\n",
        "    \n",
        "    def forward(self, imgs):\n",
        "        img_segs = self.cnn_model(imgs)\n",
        "        return img_segs"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq6u28XIwTCJ"
      },
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7-3rv4YwTCK"
      },
      "source": [
        "<img src = 'https://wikimedia.org/api/rest_v1/media/math/render/svg/80f87a71d3a616a0939f5360cec24d702d2593a2'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtTL9D0rwTCK"
      },
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = F.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        intersection = (inputs * targets).sum()                            \n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
        "        \n",
        "        return dice\n",
        "    \n",
        "    \n",
        "    \n",
        "class DiceBCELoss(nn.Module):\n",
        "    # Formula Given above.\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = F.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        intersection = (inputs * targets).mean()                            \n",
        "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.mean() + targets.mean() + smooth)  \n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        Dice_BCE = BCE + dice_loss\n",
        "        \n",
        "        return Dice_BCE.mean()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYm8juAcwTCL"
      },
      "source": [
        "## Train Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xh8396uwTCL"
      },
      "source": [
        "def HuBMAPLoss(images, targets, model, device):\n",
        "    model.to(device)\n",
        "    images = images.to(device)\n",
        "    targets = targets.to(device)\n",
        "    outputs = model(images)\n",
        "    criterion = DiceBCELoss()\n",
        "    loss = criterion(outputs, targets)\n",
        "    return loss, outputs"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGyoMBB0wTCL"
      },
      "source": [
        "def train_one_epoch(epoch, model, device, optimizer, scheduler, trainloader):\n",
        "    model.train()\n",
        "    t = time.time()\n",
        "    total_loss = 0\n",
        "    for step, (images, targets) in enumerate(trainloader):\n",
        "        loss, outputs = HuBMAPLoss(images, targets, model, device)\n",
        "        loss.backward()\n",
        "        if ((step+1)%4==0 or (step+1)==len(trainloader)):\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        loss = loss.detach().item()\n",
        "        total_loss += loss\n",
        "        if ((step+1)%10==0 or (step+1)==len(trainloader)):\n",
        "            print(\n",
        "                    f'epoch {epoch} train step {step+1}/{len(trainloader)}, ' + \\\n",
        "                    f'loss: {total_loss/len(trainloader):.4f}, ' + \\\n",
        "                    f'time: {(time.time() - t):.4f}', end= '\\r' if (step + 1) != len(trainloader) else '\\n'\n",
        "                )\n",
        "            \n",
        "    return total_loss/len(trainloader)\n",
        "\n",
        "\n",
        "def valid_one_epoch(epoch, model, device, optimizer, scheduler, validloader):\n",
        "    model.eval()\n",
        "    t = time.time()\n",
        "    total_loss = 0\n",
        "    for step, (images, targets) in enumerate(validloader):\n",
        "        loss, outputs = HuBMAPLoss(images, targets, model, device)\n",
        "        loss = loss.detach().item()\n",
        "        total_loss += loss\n",
        "        if ((step+1)%4==0 or (step+1)==len(validloader)):\n",
        "            scheduler.step(total_loss/len(validloader))\n",
        "        if ((step+1)%10==0 or (step+1)==len(validloader)):\n",
        "            print(\n",
        "                    f'epoch {epoch} valid step {step+1}/{len(validloader)}, ' + \\\n",
        "                    f'loss: {total_loss/len(validloader):.4f}, ' + \\\n",
        "                    f'time: {(time.time() - t):.4f}', end= '\\r' if (step + 1) != len(validloader) else '\\n'\n",
        "                )\n",
        "            \n",
        "    return total_loss/len(validloader)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfzApNXUwTCM"
      },
      "source": [
        "## Creating Folds Column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nNMsYKlwTCM"
      },
      "source": [
        "#FOLDS = 5 # moved to hyperparams\n",
        "gkf = GroupKFold(FOLDS)\n",
        "dir_df['Folds'] = 0\n",
        "for fold, (tr_idx, val_idx) in enumerate(gkf.split(dir_df, groups=dir_df[dir_df.columns[0]].values)):\n",
        "    dir_df.loc[val_idx, 'Folds'] = fold"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKqpaXmKwTCM"
      },
      "source": [
        "<h3>Helper</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6gFSSA-wTCN"
      },
      "source": [
        "def timer(msg, reset = False):\n",
        "    now = time.time();\n",
        "    if not hasattr(timer, \"start\") or reset:\n",
        "        timer.start = now;\n",
        "        diff = 0;\n",
        "    else:\n",
        "        diff = now - timer.start;\n",
        "        \n",
        "    print(f\"{msg} | {diff} seconds\");"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cSJwUT2wTCN"
      },
      "source": [
        "## The Real Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mptTZ3_6Dqjg"
      },
      "source": [
        "train_loss_per_epoch = []\n",
        "valid_loss_per_epoch = []\n",
        "\n",
        "fold_train_loss_dict = {}\n",
        "fold_valid_loss_dict = {}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ0-Ob5FwTCO",
        "outputId": "cd35be65-f5a0-45e5-b0ab-a1975729dee4"
      },
      "source": [
        "timer(\"starting training\", reset = True);\n",
        "for fold, (tr_idx, val_idx) in enumerate(gkf.split(dir_df, groups=dir_df[dir_df.columns[0]].values)):\n",
        "    timer(f\"{fold} fold loop\")\n",
        "    # if fold>1: # think this is just doing 1 cross validation ...\n",
        "    #     break\n",
        "        \n",
        "    trainloader, validloader = prepare_train_valid_dataloader(dir_df, [fold])\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    model = HuBMAP().to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=1)\n",
        "    \n",
        "    #num_epochs = 15\n",
        "    #num_epochs = 2 # moved to hyperparams\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # train\n",
        "        train_loss_per_epoch.append(train_one_epoch(epoch, model, device, optimizer, scheduler, trainloader))\n",
        "\n",
        "        # validate\n",
        "        with torch.no_grad():\n",
        "            valid_loss_per_epoch.append(valid_one_epoch(epoch, model, device, optimizer, scheduler, validloader))\n",
        "        \n",
        "    fold_train_loss_dict[fold] = train_loss_per_epoch\n",
        "    fold_valid_loss_dict[fold] = valid_loss_per_epoch\n",
        "    train_loss_per_epoch = []\n",
        "    valid_loss_per_epoch = []\n",
        "    \n",
        "    # save state per fold\n",
        "    torch.save(model.state_dict(), f'FOLD-{fold}-model.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting training | 0 seconds\n",
            "0 fold loop | 0.03788590431213379 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AouzQGU1vBwe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLVNovodPbQI"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-r62GkoPH1Q"
      },
      "source": [
        "# loss for epochs only\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    fig, ax1 = plt.subplots()\n",
        "    ax2 = ax1.twinx()\n",
        "    ax1.plot(fold_train_loss_dict[i], label='train', color = 'orange')\n",
        "    ax2.plot(fold_valid_loss_dict[i], label='valid', color='blue')\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Train Loss')\n",
        "    ax2.set_ylabel('Valid Loss')\n",
        "\n",
        "    plt.title(\"Loss vs Epoch for Validation Set {0}\".format(i))\n",
        "    fig.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Nfv86OHDe-b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}